{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6fbIcBnrIHZ",
        "outputId": "cbe4a40a-e935-4c0e-f24d-91e6d1df8d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.24-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from epitran) (2022.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from epitran) (57.4.0)\n",
            "Collecting panphon>=0.20\n",
            "  Downloading panphon-0.20.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from epitran) (2.25.1)\n",
            "Collecting marisa-trie\n",
            "  Downloading marisa_trie-0.7.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unicodecsv\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting munkres\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.8/dist-packages (from panphon>=0.20->epitran) (1.22.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from panphon>=0.20->epitran) (0.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from panphon>=0.20->epitran) (6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->epitran) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->epitran) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->epitran) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->epitran) (1.24.3)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10766 sha256=b3c6c68c9bf1a7fe63935a3e84b76e9db431f1abd2b5d39357099284fa39fa95\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/dd/44/ccb37563a01457f5de74ccedccaee81b01a53e12addeab5e0f\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, munkres, panphon, marisa-trie, epitran\n",
            "Successfully installed epitran-1.24 marisa-trie-0.7.8 munkres-1.1.4 panphon-0.20.0 unicodecsv-0.14.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.1.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.8/dist-packages (from pythainlp) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install epitran \n",
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpr0JsoHsBU0",
        "outputId": "b7f3fe9f-9be4-4058-afc8-b96adbe83775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn-pycrfsuite\n",
            "  Downloading sklearn-pycrfsuite-0.4.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn-pycrfsuite) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-pycrfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-pycrfsuite) (0.8.10)\n",
            "Collecting python-crfsuite-extension\n",
            "  Downloading python-crfsuite-extension-0.9.7.tar.gz (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn-pycrfsuite, python-crfsuite-extension\n",
            "  Building wheel for sklearn-pycrfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn-pycrfsuite: filename=sklearn_pycrfsuite-0.4.0-py2.py3-none-any.whl size=11002 sha256=f09d63bc97a87a29925a9b1c1d33b529191f3560b93ef34f7c9468a60bb16d05\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/58/d6/d02551a7e46a57df37e3b3422e87e4f66483155d469c8af338\n",
            "  Building wheel for python-crfsuite-extension (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-crfsuite-extension: filename=python_crfsuite_extension-0.9.7-cp38-cp38-linux_x86_64.whl size=1118937 sha256=d1abfc69143691bc6afc1c0417469a29fe0966fd9d2650f8ee11543950c89c5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/22/02/8d7e2c00573751ff96cee340de654b8d1e7532b5bcdaa88d79\n",
            "Successfully built sklearn-pycrfsuite python-crfsuite-extension\n",
            "Installing collected packages: python-crfsuite-extension, sklearn-pycrfsuite\n",
            "Successfully installed python-crfsuite-extension-0.9.7 sklearn-pycrfsuite-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn-pycrfsuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaizanJEnSni"
      },
      "outputs": [],
      "source": [
        "import pythainlp\n",
        "from pythainlp.tokenize import word_tokenize, sent_tokenize\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.corpus import thai_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "51aZfkrcp4UH",
        "outputId": "a3e1033e-47fa-47b9-bf6f-29f25b51780c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentences</th>\n",
              "      <th>words</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>clean_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>วันนี้เราไปเที่ยวสวนสัตว์ในสวนสนุก มีสัตว์เยอะ...</td>\n",
              "      <td>[วันนี้เราไปเที่ยวสวนสัตว์ในสวนสนุก , มีสัตว์เ...</td>\n",
              "      <td>[[วันนี้, เรา, ไปเที่ยว, สวนสัตว์, ใน, สวนสนุก...</td>\n",
              "      <td>[[(วันนี้, JSBR), (เรา, PPRS), (ไปเที่ยว, VACT...</td>\n",
              "      <td>[[ไปเที่ยว, สวนสัตว์, สวนสนุก,  ], [สัตว์,  , ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  วันนี้เราไปเที่ยวสวนสัตว์ในสวนสนุก มีสัตว์เยอะ...   \n",
              "\n",
              "                                           sentences  \\\n",
              "0  [วันนี้เราไปเที่ยวสวนสัตว์ในสวนสนุก , มีสัตว์เ...   \n",
              "\n",
              "                                               words  \\\n",
              "0  [[วันนี้, เรา, ไปเที่ยว, สวนสัตว์, ใน, สวนสนุก...   \n",
              "\n",
              "                                            pos_tags  \\\n",
              "0  [[(วันนี้, JSBR), (เรา, PPRS), (ไปเที่ยว, VACT...   \n",
              "\n",
              "                                         clean_words  \n",
              "0  [[ไปเที่ยว, สวนสัตว์, สวนสนุก,  ], [สัตว์,  , ...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pythainlp.tokenize import sent_tokenize, word_tokenize\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "\n",
        "# Sample Thai text\n",
        "text = 'วันนี้เราไปเที่ยวสวนสัตว์ในสวนสนุก มีสัตว์เยอะมากๆ เช่น ช้าง กระต่าย และนกฮูก'\n",
        "\n",
        "# Tokenize the text into sentences and words\n",
        "sentences = sent_tokenize(text)\n",
        "words = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# Tag the parts of speech for each word\n",
        "pos_tags = [pos_tag(word) for word in words]\n",
        "\n",
        "# # Remove stop words from the text\n",
        "stop_words = list(thai_stopwords())\n",
        "clean_words = [[word for word in sentence if word not in stop_words] for sentence in words]\n",
        "\n",
        "# Combine the results into a single DataFrame\n",
        "df = pd.DataFrame({'text': [text],\n",
        "                   'sentences': [sentences],\n",
        "                   'words': [words],\n",
        "                   'pos_tags': [pos_tags],\n",
        "                   'clean_words': [clean_words]})\n",
        "\n",
        "# Output the DataFrame\n",
        "df\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
